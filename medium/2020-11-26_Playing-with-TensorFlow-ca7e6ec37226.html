<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Playing with TensorFlow</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Playing with TensorFlow</h1>
</header>
<section data-field="subtitle" class="p-summary">
A quick literature review and example MNIST fits
</section>
<section data-field="body" class="e-content">
<section name="d9d5" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="07e5" id="07e5" class="graf graf--h3 graf--leading graf--title">Playing with TensorFlow</h3><h4 name="d2a1" id="d2a1" class="graf graf--h4 graf-after--h3 graf--subtitle">A quick literature review and example MNIST fits</h4><h3 name="0ccb" id="0ccb" class="graf graf--h3 graf-after--h4">Introduction</h3><p name="e10b" id="e10b" class="graf graf--p graf-after--h3">I’ve wanted to learn more about neural nets and in particular TensorFlow for a while now. I recently had a bit more time to dedicate to it, so began to write about what I had learned and some of the basic examples I had ran through.</p><p name="715f" id="715f" class="graf graf--p graf-after--p">Even though this information has been covered before, I decided to post it since it could help other beginners like myself. I’ve focused on parts which I thought were essential; hopefully making the subject matter as clear as possible without losing substance.</p><h3 name="a6a8" id="a6a8" class="graf graf--h3 graf-after--p">Concepts</h3><p name="b37a" id="b37a" class="graf graf--p graf-after--h3">The usual picture of a neural net as a bunch of nodes connect by lines can make the subject matter appear somewhat esoteric.</p><figure name="2140" id="2140" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Bw7EJtzjO-mHPgepr4er9w.png" data-width="693" data-height="600" src="https://cdn-images-1.medium.com/max/800/1*Bw7EJtzjO-mHPgepr4er9w.png"><figcaption class="imageCaption">An example of a multiple layer fully connected neural network. Source: Cyberbotics, via <a href="https://commons.wikimedia.org/wiki/File:Artificial_neural_network_image_recognition.png" data-href="https://commons.wikimedia.org/wiki/File:Artificial_neural_network_image_recognition.png" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">wiki</a>.</figcaption></figure><p name="7a53" id="7a53" class="graf graf--p graf-after--figure">Then you realise that the picture you have in your head does not touch the surface of neural nets used in the wild; further adding to the mystery.</p><figure name="23e5" id="23e5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FPTmBD8GY9ZvkdsUBYXC9Q.png" data-width="504" data-height="419" src="https://cdn-images-1.medium.com/max/800/1*FPTmBD8GY9ZvkdsUBYXC9Q.png"><figcaption class="imageCaption">An example of a more elaborate neural network. Source: MingxianLin, via <a href="https://commons.wikimedia.org/wiki/File:LSTM.png" data-href="https://commons.wikimedia.org/wiki/File:LSTM.png" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">wiki</a>.</figcaption></figure><p name="6f83" id="6f83" class="graf graf--p graf-after--figure">This impression is compounded when you realise the subject is a mixture of computer science, mathematics, statistics and neurology; which are already difficult subjects in their own right. All this can seem overwhelming but when you break the concepts into parts it starts to make sense.</p><h4 name="ba70" id="ba70" class="graf graf--h4 graf-after--p">Objective and Approximation</h4><p name="5e4e" id="5e4e" class="graf graf--p graf-after--h4">To start breaking this down lets focus on two concepts which I thought cut through the noise:</p><ul class="postList"><li name="a6b0" id="a6b0" class="graf graf--li graf-after--p">A neural net, no matter the complexity, is nothing more than a very complex function.</li><li name="bdc4" id="bdc4" class="graf graf--li graf-after--li">We approximate this very complex function using multiple simple non linear functions in combination.</li></ul><p name="da7f" id="da7f" class="graf graf--p graf-after--li">The <strong class="markup--strong markup--p-strong">objective</strong> of training a neural net is to find a very complex function which maps one set to another; the set and mapping completely depend on the problem you are trying to solve. <a href="https://en.wikipedia.org/wiki/Set_%28mathematics%29" data-href="https://en.wikipedia.org/wiki/Set_(mathematics)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sets</a> have a long history in mathematics which I could spend this whole article getting into. Rather than getting into that, you should see it as nothing more than mapping a series of numbers to another series of numbers. This might seem strange at first; however, it is not strange when you realise that everything can be represented by numbers: an image is nothing more than a series of numbers representing pixels; sentences can be represented by numbers, with each word being a number or a vector of numbers depending on how you want to display the information. <strong class="markup--strong markup--p-strong">All problems can be represented, in someway, by numbers</strong>.</p><p name="915f" id="915f" class="graf graf--p graf-after--p">We <strong class="markup--strong markup--p-strong">approximate</strong> this complex function by combining lots of non linear functions. The ability to approximate almost any function as a combination of non linear functions is known as the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" data-href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">universal approximation theorem</a>. For anyone familiar with Taylor or Fourier series this will make some sense; sometimes it is easier to break a function into parts with each part approximating the original function.</p><p name="a29a" id="a29a" class="graf graf--p graf-after--p">So we have the <strong class="markup--strong markup--p-strong">objective </strong>of finding a complex function which we will <strong class="markup--strong markup--p-strong">approximate </strong>using multiple non linear functions.</p><h4 name="fb32" id="fb32" class="graf graf--h4 graf-after--p">Activation Functions</h4><p name="bb41" id="bb41" class="graf graf--p graf-after--h4">These non linear functions are known as activation functions.</p><figure name="6a56" id="6a56" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*T6MAPNpDWKq3i_dHDV7YPw.png" data-width="960" data-height="720" src="https://cdn-images-1.medium.com/max/800/1*T6MAPNpDWKq3i_dHDV7YPw.png"><figcaption class="imageCaption">Structure of a single node. Source: Image by Author.</figcaption></figure><p name="498b" id="498b" class="graf graf--p graf-after--figure">Above you can see three types of parameters</p><ul class="postList"><li name="ef17" id="ef17" class="graf graf--li graf-after--p">Inputs (X).</li><li name="6f71" id="6f71" class="graf graf--li graf-after--li">Weights (W).</li><li name="5e01" id="5e01" class="graf graf--li graf-after--li">Bias (B).</li></ul><p name="914c" id="914c" class="graf graf--p graf-after--li">The <strong class="markup--strong markup--p-strong">inputs</strong> are from the data or from the previous layer. The <strong class="markup--strong markup--p-strong">weights</strong> and <strong class="markup--strong markup--p-strong">biases</strong> are what does the “<em class="markup--em markup--p-em">learning”</em>:<em class="markup--em markup--p-em"> </em>the<em class="markup--em markup--p-em"> </em>parameters we will change to produce our complex function which will solve our problem.</p><p name="47ed" id="47ed" class="graf graf--p graf-after--p">There are many different functions which can take these weights and biases as input.</p><figure name="bffe" id="bffe" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*w832H62kJ-OVRIyV7oNuFg.jpeg" data-width="896" data-height="465" src="https://cdn-images-1.medium.com/max/800/1*w832H62kJ-OVRIyV7oNuFg.jpeg"><figcaption class="imageCaption">Some activation functions. Source: Image by Author.</figcaption></figure><p name="5d9e" id="5d9e" class="graf graf--p graf-after--figure">There are <a href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" data-href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">advantages and disadvantages</a> to each function but before understanding that we need to understand how we will learn the correct weights and biases.</p><h4 name="3706" id="3706" class="graf graf--h4 graf-after--p">Loss Function</h4><p name="eb41" id="eb41" class="graf graf--p graf-after--h4">The term “<em class="markup--em markup--p-em">learning”</em> is a really abstract term which completely misses the point of what you are doing. When you “<em class="markup--em markup--p-em">learn</em>” you are setting up all the weights and biases with the correct values to map inputs to as close as possible to the known outputs. The accuracy between the calculated outputs and the known outputs, which we know in supervised learning, is measured using a loss function.</p><p name="18cf" id="18cf" class="graf graf--p graf-after--p">The best known loss function is quadratic loss which is used in least squares fitting. With this loss it is clear you are trying to get your fitting function as close to the data points as possible. However, more elaborate functions might be more difficult to visualise.</p><figure name="741c" id="741c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*OErMA5Ff6xoC5cF56Sqqug.jpeg" data-width="456" data-height="353" src="https://cdn-images-1.medium.com/max/800/1*OErMA5Ff6xoC5cF56Sqqug.jpeg"><figcaption class="imageCaption">Example of two parameters and the loss for each value. Source: Image by Author.</figcaption></figure><p name="2a50" id="2a50" class="graf graf--p graf-after--figure">Above you can see an example of a loss function for two parameters. This image is incredible useful; however, you need need to keep a few things in mind when your models get more complex.</p><ul class="postList"><li name="a7cd" id="a7cd" class="graf graf--li graf-after--p">Increase the number of dimensions for each weight and bias parameter.</li><li name="58ea" id="58ea" class="graf graf--li graf-after--li">Understand that the node types (activation functions) and how you connect them change the shape of the loss landscape.</li><li name="bcac" id="bcac" class="graf graf--li graf-after--li">The shape of the landscape is not known to you.</li></ul><p name="59ff" id="59ff" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">The last point is important. The diagram above makes you think that you could easily find the smallest value of the loss function; however, you need to find this iteratively.</strong></p><p name="63ba" id="63ba" class="graf graf--p graf-after--p">Keeping this point in mind, we then need find a method for descending the unknown loss landscape. This method follows a set procedure.</p><ol class="postList"><li name="4db9" id="4db9" class="graf graf--li graf-after--p">Initialise weights.</li><li name="12ff" id="12ff" class="graf graf--li graf-after--li">Propagate inputs through weights to get value of output.</li><li name="711d" id="711d" class="graf graf--li graf-after--li">Find value of loss function.</li><li name="4e52" id="4e52" class="graf graf--li graf-after--li">Back propagate the weights trying to minimise the loss.</li><li name="c242" id="c242" class="graf graf--li graf-after--li">Repeat.</li></ol><p name="6cb4" id="6cb4" class="graf graf--p graf-after--li">The first three parts are quite obvious to follow. The fourth part, back propagation, is the part you need to go through in a bit more detail. To get a feel for how this is done there is a good <a href="https://codesachin.wordpress.com/2015/12/06/backpropagation-for-dummies/" data-href="https://codesachin.wordpress.com/2015/12/06/backpropagation-for-dummies/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">tutorial</a> which I followed and then surmised below.</p><h4 name="d5e3" id="d5e3" class="graf graf--h4 graf-after--p">Back Propagation</h4><p name="f4a2" id="f4a2" class="graf graf--p graf-after--h4">Back propagation is using input data to iteratively update the weights. The process starts from the output layer and works backwards. To demonstrate this we will use an example activation and loss function.</p><p name="abd0" id="abd0" class="graf graf--p graf-after--p">First lets define a few terms</p><figure name="f8a4" id="f8a4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hWiahp_uZhAU5eyegmmwsA.png" data-width="716" data-height="157" src="https://cdn-images-1.medium.com/max/800/1*hWiahp_uZhAU5eyegmmwsA.png"><figcaption class="imageCaption">Source: Image by Author.</figcaption></figure><p name="b9e3" id="b9e3" class="graf graf--p graf-after--figure">We are using a quadratic loss function and sigmoid function as the activation; this would change depending on the problem.</p><p name="b036" id="b036" class="graf graf--p graf-after--p">Now we have defined our terms we can write down the update function which will change the weights iteratively:</p><figure name="ffa1" id="ffa1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dYevpqWZnTK0cRuAciqW4Q.png" data-width="716" data-height="113" src="https://cdn-images-1.medium.com/max/800/1*dYevpqWZnTK0cRuAciqW4Q.png"><figcaption class="imageCaption">The first line is the update equation. The next line is a breakdown of the the partial differential. Source: Image by Author.</figcaption></figure><p name="76f4" id="76f4" class="graf graf--p graf-after--figure">The first term of the update equation is the change of weights due to change in loss and the second is the <a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" data-href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">momentum</a> term. The only term we need to calculate is the change of weights due to change in loss. The momentum is simply the change due to the last iteration; this term is added to avoid local minimums.</p><p name="f239" id="f239" class="graf graf--p graf-after--p">Now we have the update rule, and have broken it into parts using chain rule, we need to work out the functions for each part. One of the terms is the partial derivative of the activation, the sigmoid function, relative to the inputs.</p><figure name="2105" id="2105" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BYNRaInG7B6bRi_64FzfGQ.png" data-width="716" data-height="395" src="https://cdn-images-1.medium.com/max/800/1*BYNRaInG7B6bRi_64FzfGQ.png"><figcaption class="imageCaption">Differentiation by substitution is used to determine how the activation changes relative to its inputs. Source: Image by Author.</figcaption></figure><p name="aee5" id="aee5" class="graf graf--p graf-after--figure">Other terms, which are easier to determine, are the change in inputs relative to weights and the previous layer’s activations:</p><figure name="7e97" id="7e97" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PXSwwiPab_Hc10llPykQPw.png" data-width="716" data-height="111" src="https://cdn-images-1.medium.com/max/800/1*PXSwwiPab_Hc10llPykQPw.png"><figcaption class="imageCaption">Source: Image by Author.</figcaption></figure><p name="bc97" id="bc97" class="graf graf--p graf-after--figure">Finally, we put all these partial derivative together and determine how the loss changes relative to the weights.</p><figure name="9943" id="9943" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FmotlVega58LpSCcrTZnKw.png" data-width="716" data-height="177" src="https://cdn-images-1.medium.com/max/800/1*FmotlVega58LpSCcrTZnKw.png"><figcaption class="imageCaption">Source: Image by Author.</figcaption></figure><p name="c538" id="c538" class="graf graf--p graf-after--figure">Now we need to find how the loss changes with activation.</p><figure name="8b78" id="8b78" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ev3m6TcEM_WU-1dwhZHPhg.png" data-width="716" data-height="117" src="https://cdn-images-1.medium.com/max/800/1*Ev3m6TcEM_WU-1dwhZHPhg.png"><figcaption class="imageCaption">The first equation is how the gradient of the quadratic loss changes with the data and activation for the output layer. The second equation specifies how the loss change for each layer using the result from the layer above. Source: Image by Author.</figcaption></figure><p name="2bb3" id="2bb3" class="graf graf--p graf-after--figure">Using all of this we can move back, layer after layer, determining how the weights should change. Let’s now use what we have learned to understand some typical examples.</p><h3 name="9328" id="9328" class="graf graf--h3 graf-after--p">Fully Connected Network</h3><p name="0da4" id="0da4" class="graf graf--p graf-after--h3">Using the concepts above we can begin our example fits starting with a fully connected network. Each of the examples uses the same docker image to create the required environment to run TensorFlow. I start this container with my code mounted from my local machine and allow TensorBoard to run from port 6006.</p><pre name="80ab" id="80ab" class="graf graf--pre graf-after--p">docker run <code class="markup--code markup--pre-code">-p 6006:6006 </code>-v `pwd`:/mnt/ml-mnist-examples -it tensorflow/tensorflow  bash</pre><p name="6484" id="6484" class="graf graf--p graf-after--pre">Now we have the environment we need to consider the dataset we will be playing with. The dataset I used to learn was MNIST: 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9.</p><p name="2c22" id="2c22" class="graf graf--p graf-after--p">The code I used was the same as the tutorial which you can find online.</p><figure name="db5f" id="db5f" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/mortonprod/9b1356a0532070ad24e90bac5c4d01d1.js"></script></figure><p name="73d3" id="73d3" class="graf graf--p graf-after--figure">When I first seen this I was perplexed. Even with the basic reading I had done I had to understand what the code was doing. So I went through it step by step.</p><p name="f288" id="f288" class="graf graf--p graf-after--p">The data is loaded from the normal training set. The input is an array 28x28 with each point having a number from 0 to 255. This is why we divide by 255 so each entry is between 0 and 1.</p><pre name="9cb3" id="9cb3" class="graf graf--pre graf-after--p">(x_train, y_train),(x_test, y_test) = mnist.load_data()<br>x_train, x_test = x_train / 255.0, x_test / 255.0</pre><p name="4930" id="4930" class="graf graf--p graf-after--pre">Looking at the structure of the inputs we can see it is a large 3D matrix.</p><pre name="7353" id="7353" class="graf graf--pre graf-after--p">xtrain[entry][x_pos][y_pos]<br>entry going up to 60,000<br>x_pos/y_pos going up to 27 in each direction</pre><p name="2fe9" id="2fe9" class="graf graf--p graf-after--pre">The output is nothing but a one dimensional vector specifying the value of the digit between 0 and 9.</p><pre name="6800" id="6800" class="graf graf--pre graf-after--p">y_train[entry]<br>entry going up to 60,000</pre><p name="528e" id="528e" class="graf graf--p graf-after--pre">Now we get to the setup of the neural network. The first layer is the input layer which is the same as the input image.</p><pre name="038d" id="038d" class="graf graf--pre graf-after--p">tf.keras.layers.Flatten(input_shape=(28, 28))</pre><p name="b7ab" id="b7ab" class="graf graf--p graf-after--pre">Next we have the hidden layer using the Relu activation function.</p><pre name="e7f1" id="e7f1" class="graf graf--pre graf-after--p">tf.keras.layers.Dense(512, activation=&#39;relu&#39;)</pre><p name="0b16" id="0b16" class="graf graf--p graf-after--pre">The <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" data-href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documentation</a> describes this part quite well:</p><blockquote name="6b99" id="6b99" class="graf graf--blockquote graf-after--p"><code class="markup--code markup--blockquote-code">Dense</code> implements the operation: <code class="markup--code markup--blockquote-code">output = activation(dot(input, kernel) + bias)</code> where <code class="markup--code markup--blockquote-code">activation</code> is the element-wise activation function passed as the <code class="markup--code markup--blockquote-code">activation</code> argument, <code class="markup--code markup--blockquote-code">kernel</code> is a weights matrix created by the layer, and <code class="markup--code markup--blockquote-code">bias</code> is a bias vector created by the layer (only applicable if <code class="markup--code markup--blockquote-code">use_bias</code> is <code class="markup--code markup--blockquote-code">True</code>).</blockquote><p name="44b6" id="44b6" class="graf graf--p graf-after--blockquote">So we are linking all the input pixels to 512 hidden layer nodes which will begin to workout features of the digits.</p><p name="9a86" id="9a86" class="graf graf--p graf-after--p"><a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" data-href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dropout</a>, which is used in this network, helps alleviate the problem of overfitting. Basically it sets the activations of some output nodes to zero during training. This is done to ensure the net is generalising and not just “<em class="markup--em markup--p-em">remembering</em>” the input data. Let’s set 20% of the nodes to 0 each cycle.</p><pre name="18e6" id="18e6" class="graf graf--pre graf-after--p">tf.keras.layers.Dropout(0.2)</pre><p name="f668" id="f668" class="graf graf--p graf-after--pre">Finally, we get to the output layer which uses a softmax activation function which is used for multi classification problems. This is done since all the outputs should be probabilities which sum to 100%.</p><pre name="1b84" id="1b84" class="graf graf--pre graf-after--p">tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)</pre><p name="a8ee" id="a8ee" class="graf graf--p graf-after--pre">Looking at the model summary we can see all the parameters we need to train.</p><figure name="1250" id="1250" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*H4mxgNRRINcZVMnEwC-KFg.jpeg" data-width="667" data-height="350" src="https://cdn-images-1.medium.com/max/800/1*H4mxgNRRINcZVMnEwC-KFg.jpeg"><figcaption class="imageCaption">Model summary for fully connected network. Source: Image by Author.</figcaption></figure><p name="6a7c" id="6a7c" class="graf graf--p graf-after--figure">To really understand what is going on we need to know where these ~407k parameters come from.</p><ul class="postList"><li name="0161" id="0161" class="graf graf--li graf-after--p">Flatten takes a 2D matrix and creates a 1D output, giving 28x28=784 pixels.</li><li name="6c79" id="6c79" class="graf graf--li graf-after--li">First dense layer is 512 nodes. A weight per 784 pixels which is connected to the nodes and a bias per node. Giving a total=512x784+512=401920.</li><li name="2f26" id="2f26" class="graf graf--li graf-after--li">Dropout is the same number of nodes in the layer below with no parameters to change.</li><li name="2b4b" id="2b4b" class="graf graf--li graf-after--li">Second dense layer is a weight per connection to node plus bias per node. Giving a total=512x10+10=5130.</li><li name="4167" id="4167" class="graf graf--li graf-after--li">Total=401920+5130=407050.</li></ul><p name="e152" id="e152" class="graf graf--p graf-after--li">Now we have the full network we want to do the iterative fit.</p><pre name="ae20" id="ae20" class="graf graf--pre graf-after--p">model.compile(optimizer=&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])</pre><p name="8cff" id="8cff" class="graf graf--p graf-after--pre">Adam optimiser is just a form of <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" data-href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">stochastic gradient descent</a> which specifies how we should move down the loss function. This particular loss function is used for categorisation.</p><p name="dd95" id="dd95" class="graf graf--p graf-after--p">Now specify how we want to store the output for TensorBoard which we will use later.</p><pre name="86b6" id="86b6" class="graf graf--pre graf-after--p">log_dir = &quot;logs/fit/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)</pre><pre name="8c22" id="8c22" class="graf graf--pre graf-after--pre">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)</pre><p name="2c97" id="2c97" class="graf graf--p graf-after--pre">Finally, we can run through the full dataset 5 times or epochs:</p><pre name="ac9a" id="ac9a" class="graf graf--pre graf-after--p">model.fit(x=x_train,y=y_train,epochs=5,validation_data=(x_test, y_test),callbacks=[tensorboard_callback])</pre><p name="b855" id="b855" class="graf graf--p graf-after--pre">We should have output generated and stored in logs directory. We can then look at that data using TensorBoard:</p><pre name="7c9d" id="7c9d" class="graf graf--pre graf-after--p">tensorboard --logdir logs --host 0.0.0.0</pre><h4 name="8a62" id="8a62" class="graf graf--h4 graf-after--pre">Results from TensorBoard</h4></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="8e60" id="8e60" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--h4" style="width: 49.876%;"><img class="graf-image" data-image-id="1*g7F5KyRdZ2ISKlzWkuPB7A.jpeg" data-width="360" data-height="256" src="https://cdn-images-1.medium.com/max/600/1*g7F5KyRdZ2ISKlzWkuPB7A.jpeg"></figure><figure name="1c0c" id="1c0c" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 50.124%;"><img class="graf-image" data-image-id="1*S2P4O59OLQXK-IqvJPJJeQ.jpeg" data-width="356" data-height="252" src="https://cdn-images-1.medium.com/max/600/1*S2P4O59OLQXK-IqvJPJJeQ.jpeg"><figcaption class="imageCaption" style="width: 199.505%; left: -99.505%;">Loss and accuracy for training (Orange) and validation (Blue). Source: Image by Author.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><figure name="4fbc" id="4fbc" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*YFK4v7CdDpXeRoFZWCL8Ag.jpeg" data-width="714" data-height="330" src="https://cdn-images-1.medium.com/max/800/1*YFK4v7CdDpXeRoFZWCL8Ag.jpeg"><figcaption class="imageCaption">The weights and bias for the first dense layer for each epoch. Source: Image by Author.</figcaption></figure><figure name="3739" id="3739" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*buU-M7tvh_MfhTJaEiYSxg.jpeg" data-width="725" data-height="327" src="https://cdn-images-1.medium.com/max/800/1*buU-M7tvh_MfhTJaEiYSxg.jpeg"><figcaption class="imageCaption">The weights and bias for the second dense layer for each epoch. Source: Image by Author.</figcaption></figure><h3 name="1262" id="1262" class="graf graf--h3 graf-after--figure">Recurrent Neural Network</h3><p name="287d" id="287d" class="graf graf--p graf-after--h3">I decided to then move on to use a recurrent neural network (RNN) to fit the same data. The network is not ideal for this sort of problem since they are designed for sequential data.</p><figure name="08e3" id="08e3" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/mortonprod/3646e6d1e309c3d9a08997bc25de682b.js"></script></figure><p name="029b" id="029b" class="graf graf--p graf-after--figure">This time we are reading the image data in row by row, with the network “<em class="markup--em markup--p-em">remembering</em>” details about the line above. This is, of course, not how you look at an image but in principle you could work out an image this way.</p><p name="57cd" id="57cd" class="graf graf--p graf-after--p">Looking at the network you can see it as relatively simple: a RNN layer, normalisation and then the output.</p><pre name="a996" id="a996" class="graf graf--pre graf-after--p">rnn_layer,<br>keras.layers.BatchNormalization(),<br>keras.layers.Dense(output_size),</pre><p name="0b86" id="0b86" class="graf graf--p graf-after--pre">This network has far fewer parameters than the fully connected example which you can see by looking at the model summary.</p><figure name="7513" id="7513" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2PQWsD8Po7RD51H9EirpGA.jpeg" data-width="662" data-height="298" src="https://cdn-images-1.medium.com/max/800/1*2PQWsD8Po7RD51H9EirpGA.jpeg"><figcaption class="imageCaption">Model summary of RNN. Source: Image by Author.</figcaption></figure><p name="1752" id="1752" class="graf graf--p graf-after--figure">Repeating what we did before, let’s try to understand all these parameters.</p><ul class="postList"><li name="7dcb" id="7dcb" class="graf graf--li graf-after--p">Weights for the connection between inputs and the previous RNN cell: so we have 64x64 (which is the weights linking RNN cell to the previous RNN cell), then 64x28 (which is the weights linking each RNN node to 28 pixel input), finally the biases for each node which is 64. Giving a total=64x64+64x28+64=5,952.</li><li name="815d" id="815d" class="graf graf--li graf-after--li">We have four parameters per output node of RNN for batch normalisation: two parameters for shift and scaling; two parameters for moving average and variance. Giving a total=4x64=256.</li><li name="964f" id="964f" class="graf graf--li graf-after--li">Dense layer parameters will be weights which are 64x10 plus biases for each node which sum to 10. Giving a total=64x10+10=650.</li><li name="9ce4" id="9ce4" class="graf graf--li graf-after--li">Total=5952+256+650=6858.</li></ul><h4 name="f992" id="f992" class="graf graf--h4 graf-after--li">Results from TensorBoard</h4></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="145c" id="145c" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--h4" style="width: 50.84%;"><img class="graf-image" data-image-id="1*m5jwH1m2Jkv-puvrt4DhPA.jpeg" data-width="348" data-height="230" src="https://cdn-images-1.medium.com/max/800/1*m5jwH1m2Jkv-puvrt4DhPA.jpeg"></figure><figure name="3dbe" id="3dbe" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 49.16%;"><img class="graf-image" data-image-id="1*05UPSTrIRcpDwQClgGZYdg.jpeg" data-width="354" data-height="242" src="https://cdn-images-1.medium.com/max/600/1*05UPSTrIRcpDwQClgGZYdg.jpeg"><figcaption class="imageCaption" style="width: 203.417%; left: -103.417%;">Loss and accuracy for training (Orange) and validation (Blue). Source: Image by Author.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><figure name="00c4" id="00c4" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*vdIQ_kjDy6cby1jkwzZBfA.jpeg" data-width="648" data-height="462" src="https://cdn-images-1.medium.com/max/800/1*vdIQ_kjDy6cby1jkwzZBfA.jpeg"><figcaption class="imageCaption">The weight and biases for the RNN layer. Source: Image by Author.</figcaption></figure><figure name="b5f0" id="b5f0" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*woht1wl5sGvZ0NwsqmnV2g.jpeg" data-width="665" data-height="503" src="https://cdn-images-1.medium.com/max/800/1*woht1wl5sGvZ0NwsqmnV2g.jpeg"><figcaption class="imageCaption">A look at the batch layer parameters. Source: Image by Author.</figcaption></figure><h3 name="6396" id="6396" class="graf graf--h3 graf-after--figure">Long Short-Term Memory</h3><p name="4f3d" id="4f3d" class="graf graf--p graf-after--h3">Let’s now look at a more complex RNN network known as Long Short-Term Memory (LSTM). The LSTM should allow us to “<em class="markup--em markup--p-em">remember</em>” important information we consumed many time steps before.</p><figure name="5cbc" id="5cbc" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/mortonprod/d427839ae3023032b762a351dea4f4c1.js"></script></figure><p name="c1db" id="c1db" class="graf graf--p graf-after--figure">We still don’t have as many parameter as we did for the fully connected network, but the structure of these connections is what is important with LSTM.</p><figure name="236d" id="236d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*L3QD5_kmtrRqQLpIPbrUcQ.jpeg" data-width="651" data-height="300" src="https://cdn-images-1.medium.com/max/800/1*L3QD5_kmtrRqQLpIPbrUcQ.jpeg"><figcaption class="imageCaption">Model summary LSTM. Source: Image by Author.</figcaption></figure><p name="bcd9" id="bcd9" class="graf graf--p graf-after--figure">Breaking down all the parameters from the model summary.</p><ul class="postList"><li name="1a6b" id="1a6b" class="graf graf--li graf-after--p">LSTM has the weights and biases for forget, information, candidate and output layers. Each one will need a weight for each input and hidden layer output from the last iteration. Input is 28 pixels and we specify 64 as output for LSTM, so we will have 64x64(weights)+64x28(weights)+64 (biases)=5,952. We have 4 different layers in LSTM so 5,952x4=23,808.</li><li name="cdc2" id="cdc2" class="graf graf--li graf-after--li">We have four parameters per output node for batch normalisation: two parameters for shift and scaling; two parameters for moving average and variance. Giving a total=4x64=256.</li><li name="ce82" id="ce82" class="graf graf--li graf-after--li">Dense layer will have 64x10(weights)+10(biases). Giving a total=64x10+10=650.</li><li name="e8ee" id="e8ee" class="graf graf--li graf-after--li">Total=650+256+23808=24714</li></ul><h4 name="32fd" id="32fd" class="graf graf--h4 graf-after--li">Results from TensorBoard</h4></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="eedc" id="eedc" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--h4" style="width: 50.05%;"><img class="graf-image" data-image-id="1*uW4y9eSYn3JdBJalkUkQuQ.jpeg" data-width="358" data-height="237" src="https://cdn-images-1.medium.com/max/600/1*uW4y9eSYn3JdBJalkUkQuQ.jpeg"></figure><figure name="6997" id="6997" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 49.95%;"><img class="graf-image" data-image-id="1*TUxkRK9G-Rhq08AIw2a8yw.jpeg" data-width="374" data-height="248" src="https://cdn-images-1.medium.com/max/600/1*TUxkRK9G-Rhq08AIw2a8yw.jpeg"><figcaption class="imageCaption" style="width: 200.2%; left: -100.2%;">Loss and accuracy for training (Orange) and validation (Blue). Source: Image by Author.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><figure name="7614" id="7614" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*kPVzGBNLSu_P7cew0O1TGA.jpeg" data-width="661" data-height="506" src="https://cdn-images-1.medium.com/max/800/1*kPVzGBNLSu_P7cew0O1TGA.jpeg"><figcaption class="imageCaption">Batch normalisation layer. Source: Image by Author.</figcaption></figure><figure name="c3a1" id="c3a1" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*SAELE6QDgb3cWJAPbyOvcA.jpeg" data-width="673" data-height="498" src="https://cdn-images-1.medium.com/max/800/1*SAELE6QDgb3cWJAPbyOvcA.jpeg"><figcaption class="imageCaption">All the weights and biases for LSTM layer. Source: Image by Author.</figcaption></figure><figure name="e24e" id="e24e" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*WLATCDCLHRji27KCLHQTEw.jpeg" data-width="674" data-height="261" src="https://cdn-images-1.medium.com/max/800/1*WLATCDCLHRji27KCLHQTEw.jpeg"><figcaption class="imageCaption">Weights and biases for output layer. Source: Image by Author.</figcaption></figure><h3 name="ad5c" id="ad5c" class="graf graf--h3 graf-after--figure">Convolutional Network</h3><p name="3887" id="3887" class="graf graf--p graf-after--h3">Fitting with a convolutional network should give the best results as it take into account the 2D nature of the problem.</p><figure name="80db" id="80db" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/mortonprod/7e340d570e8f086e4943e036eb425a9e.js"></script></figure><p name="1eb4" id="1eb4" class="graf graf--p graf-after--figure">This fit requires us to restructure the input data as described in this <a href="https://stackoverflow.com/questions/63109692/valueerror-input-0-of-layer-sequential-6-is-incompatible-with-the-layer-expect" data-href="https://stackoverflow.com/questions/63109692/valueerror-input-0-of-layer-sequential-6-is-incompatible-with-the-layer-expect" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">question</a>; basically, we will need to add a dimension to the input. So rather than using a single pixel grey scale we need to assume is has a multiple RGB numbers. Starting with grey scale input we had before</p><pre name="7390" id="7390" class="graf graf--pre graf-after--p">x_train[image_num][x][y] </pre><p name="7a86" id="7a86" class="graf graf--p graf-after--pre">we need to change to have the structure</p><pre name="bb84" id="bb84" class="graf graf--pre graf-after--p">x_train[image_num][x][y][0]</pre><p name="9baa" id="9baa" class="graf graf--p graf-after--pre">The last 0 is due to the assumption that we could be looking at colour photo which would have 3 entries.</p><pre name="b69a" id="b69a" class="graf graf--pre graf-after--p">x_train[image_num][x][y][0]=blue<br>x_train[image_num][x][y][1]=green <br>etc…</pre><figure name="eb4a" id="eb4a" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*zEnRkVfYTkRaJouFG0Fkqw.jpeg" data-width="655" data-height="518" src="https://cdn-images-1.medium.com/max/800/1*zEnRkVfYTkRaJouFG0Fkqw.jpeg"><figcaption class="imageCaption">Model summary of convolutional network. Source: Image by Author.</figcaption></figure><p name="e6c1" id="e6c1" class="graf graf--p graf-after--figure">Let’s look at the 2D convolution to understand the number of weights.</p><pre name="793a" id="793a" class="graf graf--pre graf-after--p">model.add(layers.Conv2D(32, (3, 3), use_bias=True, padding=&quot;SAME&quot;, activation=&#39;relu&#39;, input_shape=(28, 28, 1)))</pre><p name="c3e7" id="c3e7" class="graf graf--p graf-after--pre">The 3x3 grid is the size of a single filter, each filter has weights for each cell and bias so 3x3+1=10. We then repeat this for each filter so (9 weights + 1 bias)x32 filters=320.</p><p name="ea0f" id="ea0f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">This is for an input of 28x28x1 which is grey scale, if we had used RGB which is 28x28x3 then we would need weights for each one but with a shared bias since it’s all combined into a single feature.</strong></p><p name="9822" id="9822" class="graf graf--p graf-after--p">So the weights and biases for each filter will change to find what filters best find the input digits. So one filter might find an arc for the number 8 or a roughly 30 degree angle for the number 7.</p><p name="8ed9" id="8ed9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The size of the image does not change the number of parameters, each stride is using the same weights and bias as before. However, the depth of the input does, so we need different weights for each RGB colour but not a different bias.</strong></p><p name="4c74" id="4c74" class="graf graf--p graf-after--p">The output shape could be confusing; we have a node for each filter at each pixel stride. Note, 3x3 does not fit into 28x28 this is why we have specified</p><pre name="993e" id="993e" class="graf graf--pre graf-after--p">padding=”SAME” </pre><p name="46c5" id="46c5" class="graf graf--p graf-after--pre">So we pad in the input so 3x3 can centre on each pixel. We now have an output which lines up when a particular feature is at a particular location.</p><p name="ea38" id="ea38" class="graf graf--p graf-after--p">Max pooling is easier to understand and will run over the output of the convolution and output the maximum of the 2x2 filter it is over.</p><pre name="a87f" id="a87f" class="graf graf--pre graf-after--p">model.add(layers.MaxPooling2D((2, 2)))</pre><p name="f049" id="f049" class="graf graf--p graf-after--pre">Now we want to run a convolution over the max polling input. This will combine the features found in the earlier convolution layer to create higher order features; so curves might become circles, or angles might become multiple angles.</p><pre name="5f93" id="5f93" class="graf graf--pre graf-after--p">model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))</pre><p name="cc65" id="cc65" class="graf graf--p graf-after--pre">The number of parameter will be 3x3 weights per filter giving a total of 9. With 64 filters times 9 weights per filter we get 576 weights. Now we have all these weights per layer which was outputted by the max pool which was 32, so 32x576=18,432. Now we add the bias of all the filters which is 64 so 18,432+64=18,496.</p><p name="3982" id="3982" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Note, the output is smaller since we don’t have padding set on this layer.</strong></p><p name="5cb2" id="5cb2" class="graf graf--p graf-after--p">Running max pooling again</p><pre name="bc0a" id="bc0a" class="graf graf--pre graf-after--p">model.add(layers.MaxPooling2D((2, 2)))</pre><p name="ce0d" id="ce0d" class="graf graf--p graf-after--pre">then the last convolution</p><pre name="2c78" id="2c78" class="graf graf--pre graf-after--p">model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))</pre><p name="9ee8" id="9ee8" class="graf graf--p graf-after--pre">This will have 9 weights for each filter giving 64x9=576. We need these weights for each layer of the pooled layer below giving 576x64=36,864. Now we add the biases for each filter 36,864+64=36,928.</p><p name="868b" id="868b" class="graf graf--p graf-after--p">We then flatten the 2D structure to 1D.</p><pre name="716c" id="716c" class="graf graf--pre graf-after--p">model.add(layers.Flatten())</pre><p name="a873" id="a873" class="graf graf--p graf-after--pre">Then we have 1D dense layer with 64 nodes.</p><pre name="7560" id="7560" class="graf graf--pre graf-after--p">model.add(layers.Dense(64, activation=&#39;relu&#39;))</pre><p name="0181" id="0181" class="graf graf--p graf-after--pre">This will have weight for each input connect to the 64 nodes giving 1024x64 (weights)+64(bias)=65,600.</p><p name="61c0" id="61c0" class="graf graf--p graf-after--p">Finally we get to the output layer.</p><pre name="e97a" id="e97a" class="graf graf--pre graf-after--p">model.add(layers.Dense(10, activation=&#39;softmax&#39;))</pre><p name="748b" id="748b" class="graf graf--p graf-after--pre">This will be a weight per input to the 10 nodes and a bias per node giving 64x10+10=650</p><p name="7c18" id="7c18" class="graf graf--p graf-after--p">Putting this all together we get</p><ul class="postList"><li name="1695" id="1695" class="graf graf--li graf-after--p">Conv2D: 320.</li><li name="65ed" id="65ed" class="graf graf--li graf-after--li">Conv2D_1: 18496.</li><li name="42e7" id="42e7" class="graf graf--li graf-after--li">Conv2D_2: 36928.</li><li name="b3e8" id="b3e8" class="graf graf--li graf-after--li">Dense: 65600.</li><li name="0bed" id="0bed" class="graf graf--li graf-after--li">Dense_1: 650.</li><li name="beb1" id="beb1" class="graf graf--li graf-after--li">Total=320+18496+36928+65600+650=121994.</li></ul><h4 name="723b" id="723b" class="graf graf--h4 graf-after--li">Results from TensorBoard</h4></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="335c" id="335c" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--h4" style="width: 50.114%;"><img class="graf-image" data-image-id="1*tI08fLeBIcNt8_u8Hd1NCA.jpeg" data-width="353" data-height="230" src="https://cdn-images-1.medium.com/max/600/1*tI08fLeBIcNt8_u8Hd1NCA.jpeg"></figure><figure name="4da5" id="4da5" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 49.886%;"><img class="graf-image" data-image-id="1*eAZCB3Sn1O6P_CSfKUuhPg.jpeg" data-width="356" data-height="233" src="https://cdn-images-1.medium.com/max/600/1*eAZCB3Sn1O6P_CSfKUuhPg.jpeg"><figcaption class="imageCaption" style="width: 200.457%; left: -100.457%;">Loss and accuracy for training (Orange) and validation (Blue). Source: Image by Author.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><figure name="c057" id="c057" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*emp6HPjCZUPhJxtIVvm36w.jpeg" data-width="667" data-height="253" src="https://cdn-images-1.medium.com/max/800/1*emp6HPjCZUPhJxtIVvm36w.jpeg"><figcaption class="imageCaption">Weights and bias from first convolutional layer. Source: Image by Author.</figcaption></figure><figure name="f2be" id="f2be" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*78Fg7KCMBHUTpiQfT9qnZg.jpeg" data-width="669" data-height="270" src="https://cdn-images-1.medium.com/max/800/1*78Fg7KCMBHUTpiQfT9qnZg.jpeg"><figcaption class="imageCaption">Weights and bias from second convolutional layer. Source: Image by Author.</figcaption></figure><figure name="18a9" id="18a9" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*XKWyjFT1RAPQVgEp2i1Bag.jpeg" data-width="681" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*XKWyjFT1RAPQVgEp2i1Bag.jpeg"><figcaption class="imageCaption">Weights and bias from third convolutional layer. Source: Image by Author.</figcaption></figure><figure name="1530" id="1530" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*gKidYs2ROpIaWKral7MRww.jpeg" data-width="691" data-height="255" src="https://cdn-images-1.medium.com/max/800/1*gKidYs2ROpIaWKral7MRww.jpeg"><figcaption class="imageCaption">Weights and bias from first dense layer. Source: Image by Author.</figcaption></figure><figure name="e944" id="e944" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*7Xa5hrt0EfOAs2WeuLi_PQ.jpeg" data-width="674" data-height="255" src="https://cdn-images-1.medium.com/max/800/1*7Xa5hrt0EfOAs2WeuLi_PQ.jpeg"><figcaption class="imageCaption">Weights and bias from second dense layer. Source: Image by Author.</figcaption></figure><h3 name="19a8" id="19a8" class="graf graf--h3 graf-after--figure">Conclusion</h3><p name="069a" id="069a" class="graf graf--p graf-after--h3">The basic information provided was good enough for me to get start so hopefully will be useful to you. There are a few things I don’t cover but nothing a quick google will not provide the answer for.</p><p name="03ea" id="03ea" class="graf graf--p graf-after--p graf--trailing">Comparing the different models we get roughly what we could expect with the convolutional network doing better than the rest. There could be improvements made to the others but since this was just a learning exercise there seemed no point.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@mortonprod" class="p-author h-card">Alexander Morton</a> on <a href="https://medium.com/p/ca7e6ec37226"><time class="dt-published" datetime="2020-11-26T12:47:21.590Z">November 26, 2020</time></a>.</p><p><a href="https://medium.com/@mortonprod/playing-with-tensorflow-ca7e6ec37226" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 24, 2021.</p></footer></article></body></html>